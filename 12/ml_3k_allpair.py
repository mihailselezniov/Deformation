# -*- coding: utf-8 -*-
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, average_precision_score
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.neighbors import KNeighborsClassifier
import collections
from allpairspy_master.allpairspy import AllPairs


# -------- PAIRWISE --------
parameters = [
    list(range(10)),   #0 length
    list(range(10)),   #1 diameter
    list(range(10)),   #2 young
    list(range(10)),   #3 density
    list(range(1, 10)),#4 pressure_time
    list(range(1, 10)),#5 pressure_radius
    list(range(1, 10)),#6 pressure_amplitude
    list(range(10)),   #7 strength
]
print("PAIRWISE:")
for i, pairs in enumerate(AllPairs(parameters)):
    print("{:3d}: {}".format(i, pairs))

pairs = list(AllPairs(parameters))
# -------- END PAIRWISE --------


with open('../11/fib_all_data.txt', 'r') as f:
    data_is_broken = f.readlines()
data_is_broken = list(map(int, data_is_broken))

Y = []
for i, val in enumerate(data_is_broken):
    Y.extend([i%2]*val)

def make_str(data):
    return ''.join(map(str, data))
def make_str_float(data):
    return ''.join(map(str, map(float, data)))
def make_set(data):
    return {make_str_float(i) for i in data}

pairs = make_set(pairs)
#print(pairs)


x_train, x_test, y_train, y_test = [], [], [], []

n = tuple(map(float, range(10)))
i = 0
a = np.empty((0,8), dtype=np.float64)
for i0 in n:
    for i1 in n:
        for i2 in n:
            for i3 in n:
                for i4 in n:
                    for i5 in n:
                        for i6 in n:
                            for i7 in n:
                                if 0 not in [i4, i5, i6]:
                                    l = [i0, i1, i2, i3, i4, i5, i6, i7]
                                    ls = make_str(l)
                                    if ls in pairs:
                                        x_train.append(l)
                                        y_train.append(Y[i])
                                        print(l, Y[i])
                                    else:
                                        x_test.append(l)
                                        y_test.append(Y[i])
                                i += 1
    a = np.append(a, np.array(x_test), axis=0)
    x_test = []
    print(i0)
    #break

print('!!!')
x_train, x_test, y_train, y_test = np.array(x_train), a, np.array(y_train), np.array(y_test)
print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)
print('all', dict(collections.Counter(y_train)), dict(collections.Counter(y_test)))

# import warnings filter
from warnings import simplefilter
# ignore all future warnings
simplefilter(action='ignore', category=FutureWarning)

def fit_model(model):
    print('\n', '-'*10, model.__class__.__name__, '-'*10)
    # fit model on training data
    model.fit(x_train, y_train)
    y_pred = model.predict(x_test)
    print('y_pred', dict(collections.Counter(y_pred)))
    # make predictions for test data
    y_pred = [round(value) for value in y_pred]
    # evaluate predictions
    accuracy = accuracy_score(y_test, y_pred)
    print('Accuracy: {}'.format(accuracy))

    cm = confusion_matrix(y_test, y_pred)
    print('Confusion matrix:\n{}'.format(cm))

    print('Precision, recall and f1-score:')
    print(classification_report(y_test, y_pred))

    roc = roc_auc_score(y_test, y_pred)
    print('ROC AUC: {}'.format(roc))

    pr = average_precision_score(y_test, y_pred)
    print('Precision-recall: {}'.format(pr))

    print('-'*10, 'End',  model.__class__.__name__, '-'*10)

fit_model(XGBClassifier())
fit_model(LogisticRegression())
fit_model(LinearSVC(random_state=0, tol=1e-5))
fit_model(KNeighborsClassifier(n_neighbors=6))


'''
PAIRWISE:
  0: [0, 0, 0, 0, 1, 1, 1, 0]
  1: [1, 1, 1, 1, 2, 2, 2, 0]
  2: [2, 2, 2, 2, 3, 3, 3, 0]
  3: [3, 3, 3, 3, 4, 4, 4, 0]
  4: [4, 4, 4, 4, 5, 5, 5, 0]
  5: [5, 5, 5, 5, 6, 6, 6, 0]
  6: [6, 6, 6, 6, 7, 7, 7, 0]
  7: [7, 7, 7, 7, 8, 8, 8, 0]
  8: [8, 8, 8, 8, 9, 9, 9, 0]
  9: [9, 9, 9, 9, 9, 8, 7, 1]
 10: [9, 8, 7, 6, 6, 5, 4, 2]
 11: [8, 7, 6, 5, 5, 4, 3, 2]
 12: [7, 6, 5, 4, 4, 3, 2, 2]
 13: [6, 5, 4, 3, 3, 2, 1, 2]
 14: [5, 4, 3, 2, 2, 1, 9, 2]
 15: [4, 3, 2, 1, 1, 9, 8, 2]
 16: [3, 2, 1, 0, 8, 7, 6, 2]
 17: [2, 1, 0, 9, 7, 6, 5, 2]
 18: [1, 0, 9, 8, 7, 5, 8, 3]
 19: [0, 9, 8, 7, 6, 4, 2, 3]
 20: [0, 8, 6, 4, 3, 6, 8, 1]
 21: [1, 7, 5, 3, 1, 7, 9, 1]
 22: [2, 6, 4, 0, 9, 4, 8, 4]
 23: [3, 5, 8, 1, 5, 8, 4, 4]
 24: [4, 9, 7, 2, 4, 2, 6, 4]
 25: [5, 0, 1, 9, 4, 9, 3, 5]
 26: [6, 1, 2, 7, 5, 1, 6, 1]
 27: [7, 2, 3, 8, 1, 6, 7, 4]
 28: [8, 3, 9, 6, 2, 3, 5, 4]
 29: [9, 4, 0, 5, 8, 3, 1, 3]
 30: [3, 4, 5, 7, 3, 9, 7, 6]
 31: [9, 1, 6, 8, 4, 7, 5, 6]
 32: [8, 0, 4, 2, 6, 8, 2, 6]
 33: [7, 5, 0, 6, 9, 1, 3, 6]
 34: [6, 9, 1, 4, 1, 5, 9, 6]
 35: [5, 3, 7, 0, 7, 2, 3, 1]
 36: [4, 8, 9, 5, 2, 7, 4, 5]
 37: [2, 7, 3, 1, 6, 9, 1, 7]
 38: [1, 2, 8, 9, 8, 1, 5, 7]
 39: [0, 6, 2, 3, 8, 5, 7, 5]
 40: [5, 6, 0, 1, 3, 8, 6, 8]
 41: [3, 1, 9, 4, 6, 2, 9, 8]
 42: [0, 2, 7, 5, 9, 9, 2, 8]
 43: [1, 8, 4, 7, 4, 3, 7, 8]
 44: [2, 5, 6, 7, 2, 5, 1, 9]
 45: [4, 0, 3, 6, 3, 4, 9, 9]
 46: [6, 7, 8, 0, 2, 6, 4, 8]
 47: [7, 3, 1, 2, 5, 6, 1, 8]
 48: [8, 4, 2, 9, 7, 7, 2, 9]
 49: [9, 2, 5, 1, 7, 4, 1, 5]
 50: [5, 9, 6, 3, 9, 3, 4, 7]
 51: [9, 5, 3, 4, 8, 9, 6, 9]
 52: [8, 1, 7, 3, 1, 1, 8, 5]
 53: [7, 9, 4, 5, 7, 1, 9, 9]
 54: [2, 8, 5, 8, 5, 2, 2, 7]
 55: [0, 4, 9, 1, 4, 6, 3, 9]
 56: [3, 6, 7, 9, 2, 4, 5, 3]
 57: [6, 0, 5, 2, 9, 5, 5, 9]
 58: [4, 7, 0, 8, 6, 3, 7, 9]
 59: [1, 3, 0, 4, 9, 7, 1, 4]
 60: [3, 0, 2, 5, 1, 2, 4, 7]
 61: [0, 5, 1, 8, 7, 8, 4, 7]
 62: [2, 3, 8, 5, 3, 5, 6, 6]
 63: [7, 1, 8, 6, 8, 2, 4, 1]
 64: [9, 6, 1, 7, 1, 2, 3, 9]
 65: [5, 8, 2, 0, 5, 8, 5, 3]
 66: [1, 9, 3, 0, 3, 9, 5, 5]
 67: [4, 2, 6, 9, 6, 2, 8, 6]
 68: [6, 4, 7, 8, 8, 4, 3, 8]
 69: [8, 5, 3, 9, 4, 5, 9, 8]
 70: [4, 1, 5, 0, 9, 8, 3, 3]
 71: [5, 7, 9, 6, 1, 4, 2, 6]
 72: [7, 0, 6, 1, 8, 7, 9, 3]
 73: [2, 4, 1, 3, 6, 1, 4, 3]
 74: [3, 8, 0, 2, 7, 4, 8, 1]
 75: [8, 9, 0, 7, 2, 6, 9, 7]
 76: [9, 7, 4, 9, 3, 1, 2, 4]
 77: [6, 3, 4, 1, 6, 3, 8, 5]
 78: [1, 6, 3, 5, 5, 8, 8, 9]
 79: [0, 3, 5, 9, 2, 8, 6, 2]
 80: [7, 4, 8, 3, 7, 9, 6, 4]
 81: [8, 2, 4, 6, 4, 8, 1, 3]
 82: [3, 9, 2, 8, 2, 6, 1, 6]
 83: [2, 0, 7, 4, 5, 7, 7, 7]
 84: [1, 5, 2, 2, 1, 4, 6, 3]
 85: [9, 0, 8, 3, 2, 6, 8, 8]
 86: [5, 1, 4, 8, 3, 7, 6, 7]
 87: [4, 6, 8, 2, 8, 6, 2, 5]
 88: [0, 7, 3, 2, 9, 2, 5, 3]
 89: [6, 8, 3, 9, 1, 3, 3, 6]
 90: [8, 6, 9, 0, 6, 9, 4, 1]
 91: [7, 8, 9, 3, 5, 1, 5, 6]
 92: [3, 7, 1, 6, 9, 5, 6, 5]
 93: [6, 2, 0, 3, 5, 9, 2, 3]
 94: [5, 2, 8, 4, 2, 5, 3, 9]
 95: [2, 9, 9, 6, 8, 7, 8, 8]
 96: [7, 1, 2, 0, 6, 5, 9, 4]
 97: [4, 5, 1, 7, 7, 3, 5, 1]
 98: [1, 4, 6, 0, 1, 8, 2, 5]
 99: [8, 3, 5, 4, 8, 1, 7, 3]
100: [0, 1, 4, 2, 2, 9, 7, 1]
101: [1, 5, 7, 1, 6, 6, 3, 6]
102: [9, 3, 6, 2, 5, 3, 9, 7]
103: [0, 5, 9, 0, 4, 3, 8, 7]
104: [3, 8, 1, 1, 8, 1, 7, 4]
105: [3, 1, 4, 5, 1, 3, 3, 9]
106: [0, 4, 8, 6, 5, 7, 3, 7]
107: [7, 9, 5, 9, 5, 4, 4, 7]
108: [2, 6, 9, 7, 1, 8, 9, 5]
109: [5, 0, 8, 7, 1, 3, 7, 2]
110: [6, 6, 9, 5, 4, 1, 1, 1]
111: [8, 2, 9, 1, 3, 5, 4, 1]
112: [4, 5, 3, 7, 7, 7, 2, 1]
113: [4, 9, 7, 3, 3, 3, 3, 9]
114: [4, 4, 0, 4, 7, 2, 4, 5]
115: [4, 3, 6, 8, 1, 1, 5, 8]
116: [5, 7, 2, 4, 4, 4, 8, 8]
117: [5, 3, 9, 7, 9, 6, 4, 9]
118: [5, 4, 9, 2, 8, 7, 4, 6]
119: [8, 4, 1, 5, 3, 4, 8, 3]
120: [7, 9, 5, 1, 9, 5, 8, 2]
121: [7, 6, 9, 8, 3, 5, 2, 7]
122: [0, 1, 3, 8, 6, 4, 5, 1]
123: [0, 0, 9, 5, 7, 4, 7, 4]
124: [6, 8, 9, 0, 7, 8, 6, 9]
125: [1, 4, 5, 6, 9, 6, 4, 3]
126: [9, 5, 2, 0, 9, 3, 7, 6]
127: [0, 0, 4, 8, 8, 6, 4, 6]
128: [7, 0, 6, 8, 2, 9, 6, 5]
129: [5, 2, 9, 7, 7, 3, 6, 4]
130: [4, 4, 9, 9, 7, 2, 1, 0]
131: [8, 4, 5, 8, 7, 2, 6, 8]
132: [6, 4, 8, 8, 4, 2, 7, 7]
133: [3, 4, 6, 8, 7, 3, 2, 6]
134: [2, 4, 0, 8, 4, 5, 3, 1]
135: [4, 0, 7, 8, 6, 7, 9, 1]
136: [4, 5, 9, 8, 5, 3, 3, 5]
137: [4, 1, 9, 8, 7, 3, 1, 2]
138: [4, 6, 8, 5, 7, 3, 5, 6]
139: [4, 9, 8, 1, 7, 3, 5, 7]
140: [5, 9, 8, 4, 7, 8, 1, 0]
141: [0, 3, 5, 3, 7, 8, 2, 4]
142: [0, 2, 6, 6, 7, 9, 9, 4]
143: [6, 8, 7, 8, 7, 3, 1, 4]
144: [1, 7, 2, 6, 7, 3, 3, 4]
145: [1, 4, 9, 8, 7, 3, 3, 2]
146: [9, 4, 9, 8, 7, 3, 3, 0]

(147, 8) (72899853, 8) (147,) (72899853,)
all {1: 128, 0: 19} {1: 63518563, 0: 9381290}

 ---------- XGBClassifier ----------
y_pred {1: 65976604, 0: 6923249}
Accuracy: 0.8945657544741551
Confusion matrix:
[[ 4309199  5072091]
 [ 2614050 60904513]]
Precision, recall and f1-score:
              precision    recall  f1-score   support

           0       0.62      0.46      0.53   9381290
           1       0.92      0.96      0.94  63518563

   micro avg       0.89      0.89      0.89  72899853
   macro avg       0.77      0.71      0.73  72899853
weighted avg       0.88      0.89      0.89  72899853

ROC AUC: 0.70909279794538
Precision-recall: 0.9209906798172879
---------- End XGBClassifier ----------

 ---------- LogisticRegression ----------
y_pred {1: 64622250, 0: 8277603}
Accuracy: 0.9199868208239048
Confusion matrix:
[[ 5912972  3468318]
 [ 2364631 61153932]]
Precision, recall and f1-score:
              precision    recall  f1-score   support

           0       0.71      0.63      0.67   9381290
           1       0.95      0.96      0.95  63518563

   micro avg       0.92      0.92      0.92  72899853
   macro avg       0.83      0.80      0.81  72899853
weighted avg       0.92      0.92      0.92  72899853

ROC AUC: 0.7965333636204676
Precision-recall: 0.9435366750962181
---------- End LogisticRegression ----------

 ---------- LinearSVC ----------
/Users/mihailselezniov/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  "the number of iterations.", ConvergenceWarning)
y_pred {1: 65284428, 0: 7615425}
Accuracy: 0.922763918330535
Confusion matrix:
[[ 5683108  3698182]
 [ 1932317 61586246]]
Precision, recall and f1-score:
              precision    recall  f1-score   support

           0       0.75      0.61      0.67   9381290
           1       0.94      0.97      0.96  63518563

   micro avg       0.92      0.92      0.92  72899853
   macro avg       0.84      0.79      0.81  72899853
weighted avg       0.92      0.92      0.92  72899853

ROC AUC: 0.7876852224486599
Precision-recall: 0.941161215856892
---------- End LinearSVC ----------

 ---------- KNeighborsClassifier ----------
y_pred {1: 65894151, 0: 7005702}
Accuracy: 0.9024898719617446
Confusion matrix:
[[ 4639259  4742031]
 [ 2366443 61152120]]
Precision, recall and f1-score:
              precision    recall  f1-score   support

           0       0.66      0.49      0.57   9381290
           1       0.93      0.96      0.95  63518563

   micro avg       0.90      0.90      0.90  72899853
   macro avg       0.80      0.73      0.76  72899853
weighted avg       0.89      0.90      0.90  72899853

ROC AUC: 0.7286332869285044
Precision-recall: 0.9259223631130152
---------- End KNeighborsClassifier ----------



[0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0] 1
[0.0, 0.0, 4.0, 8.0, 8.0, 6.0, 4.0, 6.0] 1
[0.0, 0.0, 9.0, 5.0, 7.0, 4.0, 7.0, 4.0] 1
[0.0, 1.0, 3.0, 8.0, 6.0, 4.0, 5.0, 1.0] 1
[0.0, 1.0, 4.0, 2.0, 2.0, 9.0, 7.0, 1.0] 1
[0.0, 2.0, 6.0, 6.0, 7.0, 9.0, 9.0, 4.0] 1
[0.0, 2.0, 7.0, 5.0, 9.0, 9.0, 2.0, 8.0] 1
[0.0, 3.0, 5.0, 3.0, 7.0, 8.0, 2.0, 4.0] 1
[0.0, 3.0, 5.0, 9.0, 2.0, 8.0, 6.0, 2.0] 1
[0.0, 4.0, 8.0, 6.0, 5.0, 7.0, 3.0, 7.0] 1
[0.0, 4.0, 9.0, 1.0, 4.0, 6.0, 3.0, 9.0] 1
[0.0, 5.0, 1.0, 8.0, 7.0, 8.0, 4.0, 7.0] 1
[0.0, 5.0, 9.0, 0.0, 4.0, 3.0, 8.0, 7.0] 1
[0.0, 6.0, 2.0, 3.0, 8.0, 5.0, 7.0, 5.0] 1
[0.0, 7.0, 3.0, 2.0, 9.0, 2.0, 5.0, 3.0] 1
[0.0, 8.0, 6.0, 4.0, 3.0, 6.0, 8.0, 1.0] 1
[0.0, 9.0, 8.0, 7.0, 6.0, 4.0, 2.0, 3.0] 1
0.0
[1.0, 0.0, 9.0, 8.0, 7.0, 5.0, 8.0, 3.0] 1
[1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0] 1
[1.0, 2.0, 8.0, 9.0, 8.0, 1.0, 5.0, 7.0] 1
[1.0, 3.0, 0.0, 4.0, 9.0, 7.0, 1.0, 4.0] 1
[1.0, 4.0, 5.0, 6.0, 9.0, 6.0, 4.0, 3.0] 1
[1.0, 4.0, 6.0, 0.0, 1.0, 8.0, 2.0, 5.0] 1
[1.0, 4.0, 9.0, 8.0, 7.0, 3.0, 3.0, 2.0] 1
[1.0, 5.0, 2.0, 2.0, 1.0, 4.0, 6.0, 3.0] 1
[1.0, 5.0, 7.0, 1.0, 6.0, 6.0, 3.0, 6.0] 1
[1.0, 6.0, 3.0, 5.0, 5.0, 8.0, 8.0, 9.0] 1
[1.0, 7.0, 2.0, 6.0, 7.0, 3.0, 3.0, 4.0] 1
[1.0, 7.0, 5.0, 3.0, 1.0, 7.0, 9.0, 1.0] 1
[1.0, 8.0, 4.0, 7.0, 4.0, 3.0, 7.0, 8.0] 1
[1.0, 9.0, 3.0, 0.0, 3.0, 9.0, 5.0, 5.0] 1
1.0
[2.0, 0.0, 7.0, 4.0, 5.0, 7.0, 7.0, 7.0] 1
[2.0, 1.0, 0.0, 9.0, 7.0, 6.0, 5.0, 2.0] 1
[2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 0.0] 1
[2.0, 3.0, 8.0, 5.0, 3.0, 5.0, 6.0, 6.0] 1
[2.0, 4.0, 0.0, 8.0, 4.0, 5.0, 3.0, 1.0] 1
[2.0, 4.0, 1.0, 3.0, 6.0, 1.0, 4.0, 3.0] 1
[2.0, 5.0, 6.0, 7.0, 2.0, 5.0, 1.0, 9.0] 0
[2.0, 6.0, 4.0, 0.0, 9.0, 4.0, 8.0, 4.0] 1
[2.0, 6.0, 9.0, 7.0, 1.0, 8.0, 9.0, 5.0] 1
[2.0, 7.0, 3.0, 1.0, 6.0, 9.0, 1.0, 7.0] 1
[2.0, 8.0, 5.0, 8.0, 5.0, 2.0, 2.0, 7.0] 0
[2.0, 9.0, 9.0, 6.0, 8.0, 7.0, 8.0, 8.0] 1
2.0
[3.0, 0.0, 2.0, 5.0, 1.0, 2.0, 4.0, 7.0] 1
[3.0, 1.0, 4.0, 5.0, 1.0, 3.0, 3.0, 9.0] 1
[3.0, 1.0, 9.0, 4.0, 6.0, 2.0, 9.0, 8.0] 1
[3.0, 2.0, 1.0, 0.0, 8.0, 7.0, 6.0, 2.0] 1
[3.0, 3.0, 3.0, 3.0, 4.0, 4.0, 4.0, 0.0] 1
[3.0, 4.0, 5.0, 7.0, 3.0, 9.0, 7.0, 6.0] 1
[3.0, 4.0, 6.0, 8.0, 7.0, 3.0, 2.0, 6.0] 1
[3.0, 5.0, 8.0, 1.0, 5.0, 8.0, 4.0, 4.0] 1
[3.0, 6.0, 7.0, 9.0, 2.0, 4.0, 5.0, 3.0] 1
[3.0, 7.0, 1.0, 6.0, 9.0, 5.0, 6.0, 5.0] 1
[3.0, 8.0, 0.0, 2.0, 7.0, 4.0, 8.0, 1.0] 1
[3.0, 8.0, 1.0, 1.0, 8.0, 1.0, 7.0, 4.0] 1
[3.0, 9.0, 2.0, 8.0, 2.0, 6.0, 1.0, 6.0] 0
3.0
[4.0, 0.0, 3.0, 6.0, 3.0, 4.0, 9.0, 9.0] 1
[4.0, 0.0, 7.0, 8.0, 6.0, 7.0, 9.0, 1.0] 1
[4.0, 1.0, 5.0, 0.0, 9.0, 8.0, 3.0, 3.0] 1
[4.0, 1.0, 9.0, 8.0, 7.0, 3.0, 1.0, 2.0] 1
[4.0, 2.0, 6.0, 9.0, 6.0, 2.0, 8.0, 6.0] 1
[4.0, 3.0, 2.0, 1.0, 1.0, 9.0, 8.0, 2.0] 1
[4.0, 3.0, 6.0, 8.0, 1.0, 1.0, 5.0, 8.0] 1
[4.0, 4.0, 0.0, 4.0, 7.0, 2.0, 4.0, 5.0] 1
[4.0, 4.0, 4.0, 4.0, 5.0, 5.0, 5.0, 0.0] 1
[4.0, 4.0, 9.0, 9.0, 7.0, 2.0, 1.0, 0.0] 1
[4.0, 5.0, 1.0, 7.0, 7.0, 3.0, 5.0, 1.0] 1
[4.0, 5.0, 3.0, 7.0, 7.0, 7.0, 2.0, 1.0] 1
[4.0, 5.0, 9.0, 8.0, 5.0, 3.0, 3.0, 5.0] 0
[4.0, 6.0, 8.0, 2.0, 8.0, 6.0, 2.0, 5.0] 1
[4.0, 6.0, 8.0, 5.0, 7.0, 3.0, 5.0, 6.0] 1
[4.0, 7.0, 0.0, 8.0, 6.0, 3.0, 7.0, 9.0] 1
[4.0, 8.0, 9.0, 5.0, 2.0, 7.0, 4.0, 5.0] 1
[4.0, 9.0, 7.0, 2.0, 4.0, 2.0, 6.0, 4.0] 0
[4.0, 9.0, 7.0, 3.0, 3.0, 3.0, 3.0, 9.0] 0
[4.0, 9.0, 8.0, 1.0, 7.0, 3.0, 5.0, 7.0] 1
4.0
[5.0, 0.0, 1.0, 9.0, 4.0, 9.0, 3.0, 5.0] 1
[5.0, 0.0, 8.0, 7.0, 1.0, 3.0, 7.0, 2.0] 1
[5.0, 1.0, 4.0, 8.0, 3.0, 7.0, 6.0, 7.0] 1
[5.0, 2.0, 8.0, 4.0, 2.0, 5.0, 3.0, 9.0] 1
[5.0, 2.0, 9.0, 7.0, 7.0, 3.0, 6.0, 4.0] 1
[5.0, 3.0, 7.0, 0.0, 7.0, 2.0, 3.0, 1.0] 1
[5.0, 3.0, 9.0, 7.0, 9.0, 6.0, 4.0, 9.0] 1
[5.0, 4.0, 3.0, 2.0, 2.0, 1.0, 9.0, 2.0] 1
[5.0, 4.0, 9.0, 2.0, 8.0, 7.0, 4.0, 6.0] 1
[5.0, 5.0, 5.0, 5.0, 6.0, 6.0, 6.0, 0.0] 1
[5.0, 6.0, 0.0, 1.0, 3.0, 8.0, 6.0, 8.0] 1
[5.0, 7.0, 2.0, 4.0, 4.0, 4.0, 8.0, 8.0] 1
[5.0, 7.0, 9.0, 6.0, 1.0, 4.0, 2.0, 6.0] 0
[5.0, 8.0, 2.0, 0.0, 5.0, 8.0, 5.0, 3.0] 1
[5.0, 9.0, 6.0, 3.0, 9.0, 3.0, 4.0, 7.0] 0
[5.0, 9.0, 8.0, 4.0, 7.0, 8.0, 1.0, 0.0] 1
5.0
[6.0, 0.0, 5.0, 2.0, 9.0, 5.0, 5.0, 9.0] 1
[6.0, 1.0, 2.0, 7.0, 5.0, 1.0, 6.0, 1.0] 1
[6.0, 2.0, 0.0, 3.0, 5.0, 9.0, 2.0, 3.0] 1
[6.0, 3.0, 4.0, 1.0, 6.0, 3.0, 8.0, 5.0] 1
[6.0, 4.0, 7.0, 8.0, 8.0, 4.0, 3.0, 8.0] 1
[6.0, 4.0, 8.0, 8.0, 4.0, 2.0, 7.0, 7.0] 1
[6.0, 5.0, 4.0, 3.0, 3.0, 2.0, 1.0, 2.0] 0
[6.0, 6.0, 6.0, 6.0, 7.0, 7.0, 7.0, 0.0] 1
[6.0, 6.0, 9.0, 5.0, 4.0, 1.0, 1.0, 1.0] 0
[6.0, 7.0, 8.0, 0.0, 2.0, 6.0, 4.0, 8.0] 1
[6.0, 8.0, 3.0, 9.0, 1.0, 3.0, 3.0, 6.0] 0
[6.0, 8.0, 7.0, 8.0, 7.0, 3.0, 1.0, 4.0] 0
[6.0, 8.0, 9.0, 0.0, 7.0, 8.0, 6.0, 9.0] 1
[6.0, 9.0, 1.0, 4.0, 1.0, 5.0, 9.0, 6.0] 1
6.0
[7.0, 0.0, 6.0, 1.0, 8.0, 7.0, 9.0, 3.0] 1
[7.0, 0.0, 6.0, 8.0, 2.0, 9.0, 6.0, 5.0] 1
[7.0, 1.0, 2.0, 0.0, 6.0, 5.0, 9.0, 4.0] 1
[7.0, 1.0, 8.0, 6.0, 8.0, 2.0, 4.0, 1.0] 1
[7.0, 2.0, 3.0, 8.0, 1.0, 6.0, 7.0, 4.0] 1
[7.0, 3.0, 1.0, 2.0, 5.0, 6.0, 1.0, 8.0] 1
[7.0, 4.0, 8.0, 3.0, 7.0, 9.0, 6.0, 4.0] 1
[7.0, 5.0, 0.0, 6.0, 9.0, 1.0, 3.0, 6.0] 1
[7.0, 6.0, 5.0, 4.0, 4.0, 3.0, 2.0, 2.0] 0
[7.0, 6.0, 9.0, 8.0, 3.0, 5.0, 2.0, 7.0] 0
[7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 8.0, 0.0] 1
[7.0, 8.0, 9.0, 3.0, 5.0, 1.0, 5.0, 6.0] 0
[7.0, 9.0, 4.0, 5.0, 7.0, 1.0, 9.0, 9.0] 0
[7.0, 9.0, 5.0, 1.0, 9.0, 5.0, 8.0, 2.0] 1
[7.0, 9.0, 5.0, 9.0, 5.0, 4.0, 4.0, 7.0] 0
7.0
[8.0, 0.0, 4.0, 2.0, 6.0, 8.0, 2.0, 6.0] 1
[8.0, 1.0, 7.0, 3.0, 1.0, 1.0, 8.0, 5.0] 1
[8.0, 2.0, 4.0, 6.0, 4.0, 8.0, 1.0, 3.0] 1
[8.0, 2.0, 9.0, 1.0, 3.0, 5.0, 4.0, 1.0] 1
[8.0, 3.0, 5.0, 4.0, 8.0, 1.0, 7.0, 3.0] 1
[8.0, 3.0, 9.0, 6.0, 2.0, 3.0, 5.0, 4.0] 1
[8.0, 4.0, 1.0, 5.0, 3.0, 4.0, 8.0, 3.0] 1
[8.0, 4.0, 2.0, 9.0, 7.0, 7.0, 2.0, 9.0] 1
[8.0, 4.0, 5.0, 8.0, 7.0, 2.0, 6.0, 8.0] 1
[8.0, 5.0, 3.0, 9.0, 4.0, 5.0, 9.0, 8.0] 1
[8.0, 6.0, 9.0, 0.0, 6.0, 9.0, 4.0, 1.0] 1
[8.0, 7.0, 6.0, 5.0, 5.0, 4.0, 3.0, 2.0] 1
[8.0, 8.0, 8.0, 8.0, 9.0, 9.0, 9.0, 0.0] 1
[8.0, 9.0, 0.0, 7.0, 2.0, 6.0, 9.0, 7.0] 1
8.0
[9.0, 0.0, 8.0, 3.0, 2.0, 6.0, 8.0, 8.0] 1
[9.0, 1.0, 6.0, 8.0, 4.0, 7.0, 5.0, 6.0] 1
[9.0, 2.0, 5.0, 1.0, 7.0, 4.0, 1.0, 5.0] 1
[9.0, 3.0, 6.0, 2.0, 5.0, 3.0, 9.0, 7.0] 1
[9.0, 4.0, 0.0, 5.0, 8.0, 3.0, 1.0, 3.0] 1
[9.0, 4.0, 9.0, 8.0, 7.0, 3.0, 3.0, 0.0] 1
[9.0, 5.0, 2.0, 0.0, 9.0, 3.0, 7.0, 6.0] 1
[9.0, 5.0, 3.0, 4.0, 8.0, 9.0, 6.0, 9.0] 1
[9.0, 6.0, 1.0, 7.0, 1.0, 2.0, 3.0, 9.0] 0
[9.0, 7.0, 4.0, 9.0, 3.0, 1.0, 2.0, 4.0] 0
[9.0, 8.0, 7.0, 6.0, 6.0, 5.0, 4.0, 2.0] 1
[9.0, 9.0, 9.0, 9.0, 9.0, 8.0, 7.0, 1.0] 1
9.0
'''
