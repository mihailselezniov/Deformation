import multiprocessing

def calc_distance(pi, threads, threads_arr, result_dis_workers, result_ids_workers):
    for tid, l0 in threads_arr:
        dis = min([math.sqrt(sum(map(lambda x: (x[0] - x[1])**2, zip(thread, l0)))) for thread in threads])
        if dis > result_dis_workers[pi]:
            result_dis_workers[pi] = dis
            result_ids_workers[pi] = tid



if __name__ == "__main__":

    import numpy as np
    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, average_precision_score
    from xgboost import XGBClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import LinearSVC
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.externals import joblib
    import collections
    import math
    import sys
    import time
    import pickle


    ml_type = int(sys.argv[1])

    '''
    threads = {
        0: [[2,5,6,7,2,5,1,9],[3,9,2,8,2,6,1,6],[4,9,7,3,3,3,3,9],[5,7,9,6,1,4,2,6],[6,5,4,3,3,2,1,2],[7,6,5,4,4,3,2,2],[9,6,1,7,1,2,3,9]],
        1: [[0,0,0,0,1,1,1,0],[2,0,7,4,5,7,7,7],[4,0,3,6,3,4,9,9],[5,8,2,0,5,8,5,3],[6,4,8,8,4,2,7,7],[7,5,0,6,9,1,3,6],[8,4,1,5,3,4,8,3]]
    }
    joblib.dump(threads, 'threads_{}.j'.format(ml_type))
    '''
    threads_f = 'threads/threads_{}'.format(ml_type)
    threads_f_name = '{}.j'.format(threads_f)
    threads72_f_name = '{}_72.j'.format(threads_f)
    threads729_f_name = '{}_729.j'.format(threads_f)
    threads = joblib.load(threads_f_name)
    len_threads = len(threads[0]) + len(threads[1])
    print('threads', len_threads)
    print(sys.getsizeof(threads))

    #way_dict = joblib.load('border_vars/way_dict.j')
    #with open('border_vars/way_dict.p', 'rb') as f:
    #    way_dict = pickle.load(f)

    #print('way_dict')
    #way_dict = list(way_dict)
    #joblib.dump(way_dict, 'border_vars/way_dict2.j')

    #print(len(way_dict), way_dict[0])
    #with open('border_vars/way_dict.txt', 'a') as f:
    #    for i in way_dict:
    #        f.write(','.join(list(map(str, i)))+'\n')

    with open('border_vars/way_dict.txt', 'r') as f:
        way = f.readlines()
    print('way')
    #print(way[0])
    #print(way[1])
    #print(way[2])
    #way_dict = []

    #for row in way:
    #    way_dict.append(tuple(map(int, row.split(','))))
    #print(len(way_dict), way_dict[0])





    #print('way_dict')
    X = joblib.load('border_vars/X.j')
    print('X')
    Y = joblib.load('border_vars/Y.j')
    print('Y')

    #print(sys.getsizeof(way_dict))
    print(X.shape, Y.shape)
    print('all', dict(collections.Counter(Y)))
    print(sys.getsizeof(X), sys.getsizeof(Y))


    # import warnings filter
    from warnings import simplefilter
    # ignore all future warnings
    simplefilter(action='ignore', category=FutureWarning)


    x_train, y_train = [], []
    model = ''
    if ml_type == 1:
        model = XGBClassifier()
    elif ml_type == 2:
        model = LogisticRegression()
    elif ml_type == 3:
        model = LinearSVC(random_state=0, tol=1e-5)
    elif ml_type == 4:
        model = KNeighborsClassifier(n_neighbors=1)

    def gen_train():
        global x_train
        global y_train
        x_train, y_train = [], []
        for i in threads:
            for thread in threads[i]:
                x_train.append(thread)
                y_train.append(i)
        x_train, y_train = np.array(x_train), np.array(y_train)

    while len_threads < 729:
        print('#', len_threads)
        gen_train()

        model.fit(x_train, y_train)
        y_pred = model.predict(X)
        accuracy = accuracy_score(Y, y_pred)
        print('Accuracy: {}'.format(accuracy))
        #print(y_pred)
        print(dict(collections.Counter(y_pred)))

        find = 1 if len(threads[0]) > len(threads[1]) else 0

        num_proc = 6
        threads_arr = {}
        for i in range(num_proc):
            threads_arr[i] = []


        pi_id = 0
        for i in range(len(y_pred)):
            if y_pred[i] == 0:
                #for j in way_dict[i]:
                for j in map(int, way[i].split(',')):
                    if y_pred[j] == 1:
                        t_id = j if find else i
                        threads_arr[pi_id%num_proc].append([t_id, list(X[t_id])])
                        pi_id += 1


        result_dis_workers = multiprocessing.Array('d', num_proc)
        result_ids_workers = multiprocessing.Array('i', num_proc)

        for i in range(num_proc):
            result_dis_workers[i] = 0

        ps = []
        for pi in range(num_proc):
            ps.append(multiprocessing.Process(target=calc_distance, args=(pi, threads[0], threads_arr[pi], result_dis_workers, result_ids_workers)))

        for p in ps:
            p.start()

        for p in ps:
            p.join()

        tr = sorted(list(zip(result_dis_workers[:], result_ids_workers[:])), reverse=True)[0]
        print(tr)
        max_dis_thread = tr[1]

        #print([max_distance, max_dis_thread])
        print(Y[max_dis_thread])
        threads[Y[max_dis_thread]].append(list(X[max_dis_thread]))
        len_threads = len(threads[0]) + len(threads[1])
        if len_threads == 72:
            joblib.dump(threads, threads72_f_name)
        elif len_threads == 729:
            joblib.dump(threads, threads729_f_name)
        else:
            threadsN_f_name = '{}_{}.j'.format(threads_f, len_threads)
            joblib.dump(threads, threadsN_f_name)
        joblib.dump(threads, threads_f_name)
        #print(threads)
        #break





'''
(72900000, 8) (72900000,)
all {1: 63518691, 0: 9381309}
# 14
Accuracy: 0.6998067352537722
{1: 43837200, 0: 29062800}
[14.2828568570857, 5102990]
1
# 15
Accuracy: 0.8173903017832648
{1: 55836000, 0: 17064000}
[13.74772708486752, 728920]
1
# 16
Accuracy: 0.8739810562414266
{1: 61618500, 0: 11281500}
[12.806248474865697, 6568210]
1
# 17
Accuracy: 0.7952048148148149
{1: 59262840, 0: 13637160}
[13.674794331177344, 6563420]
1
# 18
Accuracy: 0.8727472290809328
{1: 61503690, 0: 11396310}
[12.083045973594572, 66196359]
1
# 19
Accuracy: 0.8599926886145405
{1: 60287000, 0: 12613000}
[12.569805089976535, 4737775]
1
# 20
Accuracy: 0.8837192181069958
{1: 64671600, 0: 8228400}
[11.958260743101398, 14507180]
1
# 21
Accuracy: 0.8887181481481482
{1: 65633496, 0: 7266504}
[12.0, 65902329]
1
# 22
Accuracy: 0.901887146776406
{1: 66510792, 0: 6389208}
[11.489125293076057, 7289200]
1
# 23
Accuracy: 0.8843528532235939
{1: 67988760, 0: 4911240}
[10.816653826391969, 72173348]
1
# 24
Accuracy: 0.8747271742112482
{1: 67849660, 0: 5050340}
[11.224972160321824, 65890]
1
# 25
Accuracy: 0.8746583676268861
{1: 65884988, 0: 7015012}
[11.489125293076057, 19019780]
1
# 26
Accuracy: 0.900230329218107
{1: 67433814, 0: 5466186}
[10.583005244258363, 71951949]
0
# 27
Accuracy: 0.9030360356652949
{1: 67006394, 0: 5893606}
[11.090536506409418, 71448669]
0
# 28
Accuracy: 0.8997987791495199
{1: 67240612, 0: 5659388}
[12.041594578792296, 66272769]
1
# 29
Accuracy: 0.8939306310013717
{1: 66757940, 0: 6142060}
[11.74734012447073, 26381969]
1
# 30
Accuracy: 0.9056758984910837
{1: 66176598, 0: 6723402}
[11.357816691600547, 6560641]
0
# 31
Accuracy: 0.9058410013717421
{1: 65053670, 0: 7846330}
[11.832159566199232, 4599905]
1
# 32
Accuracy: 0.9102788614540467
{1: 66562280, 0: 6337720}
[11.045361017187261, 6582511]
1
# 33
Accuracy: 0.8981539780521262
{1: 66075660, 0: 6824340}
[10.816653826391969, 65026710]
1




threads 14
248
way
X
Y
(72900000, 8) (72900000,)
all {1: 63518691, 0: 9381309}
4665600112 583200096
# 14
Accuracy: 0.6998067352537722
{1: 43837200, 0: 29062800}
(14.2828568570857, 5102990)
1
# 15
Accuracy: 0.8173903017832648
{1: 55836000, 0: 17064000}
(13.74772708486752, 728920)
1
# 16
Accuracy: 0.8739810562414266
{1: 61618500, 0: 11281500}
(12.806248474865697, 6568210)
1
# 17
Accuracy: 0.7952048148148149
{1: 59262840, 0: 13637160}
(13.674794331177344, 6563420)
1
# 18
Accuracy: 0.8727472290809328
{1: 61503690, 0: 11396310}
(12.083045973594572, 66196359)
1
# 19
Accuracy: 0.8599926886145405
{1: 60287000, 0: 12613000}
(12.569805089976535, 4737775)
1
# 20
Accuracy: 0.8837192181069958
{1: 64671600, 0: 8228400}
(11.958260743101398, 14507180)
1
# 21
Accuracy: 0.8887181481481482
{1: 65633496, 0: 7266504}
(12.0, 65902339)
1
# 22
Accuracy: 0.901887146776406
{1: 66510792, 0: 6389208}
(11.489125293076057, 7289200)
1
# 23
Accuracy: 0.8843528532235939
{1: 67988760, 0: 4911240}
(10.816653826391969, 72892880)
1
# 24
Accuracy: 0.8954786694101509
{1: 67976400, 0: 4923600}
(11.135528725660043, 72177499)
0
# 25
Accuracy: 0.9020443758573388
{1: 66077080, 0: 6822920}
(11.40175425099138, 72754129)
0
# 26
Accuracy: 0.897152962962963
{1: 65528240, 0: 7371760}
(12.409673645990857, 4381219)
1
# 27
Accuracy: 0.8881279698216735
{1: 65228160, 0: 7671840}
(11.874342087037917, 21148200)
1
# 28
Accuracy: 0.898409451303155
{1: 65941966, 0: 6958034}
(12.24744871391589, 66330919)
1
# 29
Accuracy: 0.8941714814814815
{1: 65502020, 0: 7397980}
(11.357816691600547, 57608008)
1
# 30
Accuracy: 0.8981090534979423
{1: 66253315, 0: 6646685}
(10.723805294763608, 72171736)
0
# 31
Accuracy: 0.8966196021947874
{1: 65742510, 0: 7157490}
(10.862780491200215, 70785811)
1
# 32
Accuracy: 0.9068137311385459
{1: 66666724, 0: 6233276}
(11.090536506409418, 7289642)
0
# 33
Accuracy: 0.9018167215363512
{1: 65109710, 0: 7790290}
(11.789826122551595, 6568032)
1
# 34
Accuracy: 0.9044673525377229
{1: 65623819, 0: 7276181}
(11.445523142259598, 6567944)
1
# 35
Accuracy: 0.9021080109739369
{1: 65390851, 0: 7509149}
(11.090536506409418, 72316256)
1
# 36
Accuracy: 0.9081512757201646
{1: 65900363, 0: 6999637}
(10.677078252031311, 21294009)
1
# 37
Accuracy: 0.9077128943758573
{1: 65909727, 0: 6990273}
(10.723805294763608, 6999132)
0
# 38
Accuracy: 0.9064225102880659
{1: 66042562, 0: 6857438}
(10.535653752852738, 72834324)
1
# 39
Accuracy: 0.90298451303155
{1: 66625616, 0: 6274384}
(10.295630140987, 6603949)
1
# 40
Accuracy: 0.9015166666666666
{1: 66762768, 0: 6137232}
(10.246950765959598, 66190058)
1
# 41
Accuracy: 0.8986953497942387
{1: 67416708, 0: 5483292}
(10.14889156509222, 69918039)
0
# 42
Accuracy: 0.9019403703703703
{1: 66431578, 0: 6468422}
(11.090536506409418, 26972549)
1
# 43
Accuracy: 0.9002668312757202
{1: 66507115, 0: 6392885}
(10.099504938362077, 7077873)
0
# 44
Accuracy: 0.9076890260631001
{1: 66391405, 0: 6508595}
(10.099504938362077, 3645009)
0
# 45
Accuracy: 0.9078513991769548
{1: 66338622, 0: 6561378}
(10.099504938362077, 66208599)
1
# 46
Accuracy: 0.9026982304526749
{1: 65721122, 0: 7178878}
(9.899494936611665, 72243542)
1
# 47
Accuracy: 0.9036157475994513
{1: 65670989, 0: 7229011}
(9.848857801796104, 4221630)
1
# 48
Accuracy: 0.9088207956104253
{1: 66853049, 0: 6046951}
(10.04987562112089, 57591899)
1
# 49
Accuracy: 0.9086762551440329
{1: 66779946, 0: 6120054}
(9.746794344808963, 6517179)
0
# 50
Accuracy: 0.9099509053497943
{1: 66498500, 0: 6401500}
(9.9498743710662, 6589639)
1
# 51
Accuracy: 0.9108080109739369
{1: 66845041, 0: 6054959}
(9.695359714832659, 48113199)
0
# 52
Accuracy: 0.9145081618655693
{1: 66805248, 0: 6094752}
(9.746794344808963, 5218929)
0
# 53
Accuracy: 0.9192490672153635
{1: 66128032, 0: 6771968}
(9.643650760992955, 72315995)
0
# 54
Accuracy: 0.9201759396433471
{1: 65356273, 0: 7543727}
(9.848857801796104, 34350395)
1
# 55
Accuracy: 0.9188175994513031
{1: 66575714, 0: 6324286}
(9.539392014169456, 72827822)
0
# 56
Accuracy: 0.9173743621399177
{1: 66579358, 0: 6320642}
(9.695359714832659, 72895870)
1
# 57
Accuracy: 0.9222681207133059
{1: 67020973, 0: 5879027}
(9.486832980505138, 72747640)
1
# 58
Accuracy: 0.9228706310013718
{1: 66810574, 0: 6089426}
(10.198039027185569, 678422)
1
# 59
Accuracy: 0.9258103978052126
{1: 66742327, 0: 6157673}
(9.433981132056603, 6335010)
1
# 60
Accuracy: 0.9236810288065843
{1: 67312160, 0: 5587840}
(9.38083151964686, 50672702)
1
# 61
Accuracy: 0.9205431687242799
{1: 67329904, 0: 5570096}
(9.38083151964686, 46438039)
1
# 62
Accuracy: 0.921719122085048
{1: 67732655, 0: 5167345}
(9.16515138991168, 69980672)
0
# 63
Accuracy: 0.9210515226337449
{1: 67109835, 0: 5790165}
(9.16515138991168, 33467492)
1
# 64
Accuracy: 0.9159278737997256
{1: 66829115, 0: 6070885}
(9.539392014169456, 13927869)
1
# 65
Accuracy: 0.917652647462277
{1: 67411157, 0: 5488843}
(9.539392014169456, 72674548)
1
# 66
Accuracy: 0.9196945404663923
{1: 67576005, 0: 5323995}
(9.16515138991168, 69321342)
1
# 67
Accuracy: 0.9192676406035666
{1: 67673516, 0: 5226484}
(9.1104335791443, 3933362)
0
# 68
Accuracy: 0.9217236762688614
{1: 67286887, 0: 5613113}
(9.0, 50752903)
1
# 69
Accuracy: 0.9190054183813443
{1: 67703772, 0: 5196228}
(9.16515138991168, 7238970)
0
# 70
Accuracy: 0.9202483401920439
{1: 67545341, 0: 5354659}
(8.94427190999916, 6563632)
1
# 71
Accuracy: 0.9217535390946502
{1: 67456550, 0: 5443450}
(9.848857801796104, 66216709)
1
# 72
Accuracy: 0.9218303017832647
{1: 67783180, 0: 5116820}
(9.055385138137417, 6569029)
0
# 73
Accuracy: 0.9211727846364883
{1: 68104541, 0: 4795459}
(9.591663046625438, 71660879)
1
# 74
Accuracy: 0.9221676131687243
{1: 67949244, 0: 4950756}
(8.94427190999916, 4367432)
0
# 75
Accuracy: 0.922230877914952
{1: 68025380, 0: 4874620}
(8.831760866327848, 50249882)
0
# 76
Accuracy: 0.920218353909465
{1: 67921431, 0: 4978569}
(9.38083151964686, 72899190)
1
# 77
Accuracy: 0.9201464883401921
{1: 67725910, 0: 5174090}
(9.055385138137417, 62795989)
1
# 78
Accuracy: 0.9203960631001372
{1: 68077960, 0: 4822040}
(8.774964387392123, 6575782)
1
# 79
Accuracy: 0.9191874897119342
{1: 67677391, 0: 5222609}
(8.717797887081348, 3904922)
1
# 80
Accuracy: 0.9151655829903979
{1: 68019414, 0: 4880586}
(8.717797887081348, 11591829)
0
# 81
Accuracy: 0.9165895747599452
{1: 68131389, 0: 4768611}
(8.717797887081348, 6563612)
1
# 82
Accuracy: 0.9181159945130315
{1: 67813605, 0: 5086395}
(8.660254037844387, 69379662)
1
# 83
Accuracy: 0.9184171879286694
{1: 68352986, 0: 4547014}
(8.660254037844387, 69182109)
0
# 84
Accuracy: 0.923341207133059
{1: 68199329, 0: 4700671}
(8.831760866327848, 68985999)
0
# 85
Accuracy: 0.9236753635116598
{1: 67626229, 0: 5273771}
(8.660254037844387, 72899292)
0
# 86
Accuracy: 0.9225066255144033
{1: 67709638, 0: 5190362}
(8.660254037844387, 72833593)
0
# 87
Accuracy: 0.9222519067215363
{1: 67725301, 0: 5174699}
(8.602325267042627, 3599649)
0
# 88
Accuracy: 0.9226117283950618
{1: 67888688, 0: 5011312}
(8.660254037844387, 68585855)
1
# 89
Accuracy: 0.9245703840877915
{1: 67729924, 0: 5170076}
(8.602325267042627, 6582882)
0
# 90
Accuracy: 0.9225800274348422
{1: 67784461, 0: 5115539}
(8.54400374531753, 68532492)
1
# 91
Accuracy: 0.924658024691358
{1: 67603777, 0: 5296223}
(8.48528137423857, 54863378)
1
# 92
Accuracy: 0.919891426611797
{1: 68176022, 0: 4723978}
(8.426149773176359, 35749442)
0
# 93
Accuracy: 0.9201333196159122
{1: 68251506, 0: 4648494}
(8.306623862918075, 64946612)
0
# 94
Accuracy: 0.9194311659807957
{1: 68512639, 0: 4387361}
(8.54400374531753, 45443529)
0
# 95
Accuracy: 0.9215826063100138
{1: 68152997, 0: 4747003}
(9.539392014169456, 808383)
1
# 96
Accuracy: 0.9230846639231824
{1: 67978337, 0: 4921663}
(9.1104335791443, 2133182)
1
# 97
Accuracy: 0.9200359122085048
{1: 68432253, 0: 4467747}
(8.54400374531753, 70108652)
0
# 98
Accuracy: 0.9201860082304527
{1: 68271943, 0: 4628057}
(8.660254037844387, 38621529)
1
# 99
Accuracy: 0.9193924005486969
{1: 68188579, 0: 4711421}
(8.306623862918075, 57591017)
0
# 100
Accuracy: 0.9215469684499314
{1: 68098763, 0: 4801237}
(8.306623862918075, 3674882)
0
# 101
Accuracy: 0.921911426611797
{1: 67994384, 0: 4905616}
(8.831760866327848, 72871109)
0
# 102
Accuracy: 0.9234131001371743
{1: 68217740, 0: 4682260}
(10.488088481701515, 7268939)
1
# 103
Accuracy: 0.9226942935528121
{1: 68206607, 0: 4693393}
(9.486832980505138, 72687059)
0
# 104
Accuracy: 0.9194900960219479
{1: 68203249, 0: 4696751}
(9.539392014169456, 7099739)
1
# 105
Accuracy: 0.9200433607681756
{1: 68093710, 0: 4806290}
(9.643650760992955, 72681382)
0
# 106
Accuracy: 0.921709451303155
{1: 67792424, 0: 5107576}
(9.899494936611665, 70531552)
1
# 107
Accuracy: 0.9233479561042524
{1: 68022651, 0: 4877349}
(9.055385138137417, 72753478)
0
# 108
Accuracy: 0.9258918518518519
{1: 67631573, 0: 5268427}
(9.9498743710662, 34225822)
1
# 109
Accuracy: 0.9216332098765432
{1: 68042964, 0: 4857036}
(9.643650760992955, 72278812)
1
# 110
Accuracy: 0.9235297393689986
{1: 68153771, 0: 4746229}
(8.94427190999916, 72682107)
1
# 111
Accuracy: 0.9227889163237312
{1: 68301403, 0: 4598597}
(8.426149773176359, 72748602)
0
# 112
Accuracy: 0.9260837997256516
{1: 67404590, 0: 5495410}
(9.327379053088816, 7283598)
0
# 113
Accuracy: 0.9246060768175584
{1: 66284202, 0: 6615798}
(9.591663046625438, 4811474)
1
# 114
Accuracy: 0.9250857338820302
{1: 67052765, 0: 5847235}
(9.327379053088816, 7247867)
1
# 115
Accuracy: 0.9273399314128944
{1: 67442248, 0: 5457752}
(9.0, 72683719)
1
# 116
Accuracy: 0.929492304526749
{1: 67618320, 0: 5281680}
(8.888194417315589, 7072994)
1
# 117
Accuracy: 0.9257175582990398
{1: 68194363, 0: 4705637}
(8.888194417315589, 67646409)
1
# 118
Accuracy: 0.9261461728395062
{1: 68048325, 0: 4851675}
(8.246211251235321, 49818158)
0
# 119
Accuracy: 0.9297659807956105
{1: 67699479, 0: 5200521}
(8.660254037844387, 50592512)
1
# 120
Accuracy: 0.9280992043895747
{1: 67835939, 0: 5064061}
(8.48528137423857, 29013462)
1
# 121
Accuracy: 0.9289603566529493
{1: 67528911, 0: 5371089}
(8.306623862918075, 72863002)
1
# 122
Accuracy: 0.926772962962963
{1: 67594140, 0: 5305860}
(8.831760866327848, 72219684)
1
# 123
Accuracy: 0.926330109739369
{1: 68105086, 0: 4794914}
(8.306623862918075, 3499216)
0
# 124
Accuracy: 0.9279102743484225
{1: 67876196, 0: 5023804}
(8.18535277187245, 25369027)
1
# 125
Accuracy: 0.9277414128943758
{1: 67905608, 0: 4994392}
(8.12403840463596, 7216509)
0
# 126
Accuracy: 0.931328573388203
{1: 67343224, 0: 5556776}
(8.366600265340756, 3710649)
1
# 127
Accuracy: 0.930883086419753
{1: 67423502, 0: 5476498}
(8.246211251235321, 72747669)
0
# 128
Accuracy: 0.9313326474622771
{1: 66853347, 0: 6046653}
(8.18535277187245, 12028418)
1
# 129
Accuracy: 0.9320416186556927
{1: 66897453, 0: 6002547}
(8.246211251235321, 69036389)
1
# 130
Accuracy: 0.9304822770919067
{1: 67039371, 0: 5860629}
(8.06225774829855, 6677689)
0
# 131
Accuracy: 0.9293745404663923
{1: 65929107, 0: 6970893}
(8.246211251235321, 19658743)
0
# 132
Accuracy: 0.9293180795610425
{1: 65094343, 0: 7805657}
(8.18535277187245, 14575907)
1
# 133
Accuracy: 0.934008134430727
{1: 65587488, 0: 7312512}
(8.18535277187245, 7283478)
0
# 134
Accuracy: 0.9336516323731139
{1: 65756071, 0: 7143929}
(8.18535277187245, 65540299)
1
# 135
Accuracy: 0.9322506721536351
{1: 66195391, 0: 6704609}
(8.306623862918075, 2165132)
0
# 136
Accuracy: 0.9327138957475994
{1: 66082554, 0: 6817446}
(8.660254037844387, 67789712)
0
# 137
Accuracy: 0.93408670781893
{1: 65734152, 0: 7165848}
(8.48528137423857, 31289312)
1
# 138
Accuracy: 0.9354897530864198
{1: 65782490, 0: 7117510}
(8.774964387392123, 102063)
1
# 139
Accuracy: 0.934045596707819
{1: 65772525, 0: 7127475}
(8.366600265340756, 67702279)
1
# 140
Accuracy: 0.9340281755829904
{1: 66256765, 0: 6643235}
(8.246211251235321, 67767033)
0
# 141
Accuracy: 0.9357800411522633
{1: 65843834, 0: 7056166}
(9.0, 1535763)
1
# 142
Accuracy: 0.933793182441701
{1: 66089002, 0: 6810998}
(8.18535277187245, 23379036)
0
# 143
Accuracy: 0.933898573388203
{1: 66393719, 0: 6506281}
(8.660254037844387, 67110932)
1
# 144
Accuracy: 0.9337289986282579
{1: 66163687, 0: 6736313}
(8.18535277187245, 23989052)
1
# 145
Accuracy: 0.9342166117969821
{1: 66455272, 0: 6444728}
(8.06225774829855, 72862822)
0
# 146
Accuracy: 0.9369265843621399
{1: 66381063, 0: 6518937}
(8.18535277187245, 23357889)
1
# 147
Accuracy: 0.9351089437585733
{1: 66209851, 0: 6690149}
(8.06225774829855, 72215635)
1
# 148
Accuracy: 0.9339388065843621
{1: 66432320, 0: 6467680}
(8.306623862918075, 14507983)
0
# 149
Accuracy: 0.9369233470507544
{1: 66146391, 0: 6753609}
(8.06225774829855, 21571152)
0
# 150
Accuracy: 0.9366645541838134
{1: 65910815, 0: 6989185}
(8.06225774829855, 68520379)
1
# 151
Accuracy: 0.9355775994513031
{1: 65918646, 0: 6981354}
(8.426149773176359, 58247879)
1
# 152
Accuracy: 0.9340443758573388
{1: 65890766, 0: 7009234}
(8.0, 33822408)
0
# 153
Accuracy: 0.9355666941015089
{1: 65514781, 0: 7385219}
(8.06225774829855, 72397079)
1
# 154
Accuracy: 0.9352802469135802
{1: 65764933, 0: 7135067}
(8.0, 70136322)
1
# 155
Accuracy: 0.9367877503429355
{1: 65950714, 0: 6949286}
(7.937253933193772, 72827100)
1
# 156
Accuracy: 0.9381673936899863
{1: 65646642, 0: 7253358}
(7.937253933193772, 58247869)
0
# 157
Accuracy: 0.9342520987654321
{1: 65583213, 0: 7316787}
(7.937253933193772, 43500192)
1
# 158
Accuracy: 0.9344288340192044
{1: 65805957, 0: 7094043}
(8.18535277187245, 7237438)
0
# 159
Accuracy: 0.9340417695473251
{1: 65691304, 0: 7208696}
(8.0, 19176796)
1
# 160
Accuracy: 0.9334599862825789
{1: 65955316, 0: 6944684}
(8.18535277187245, 52618502)
1
# 161
Accuracy: 0.9319510288065843
{1: 66233361, 0: 6666639}
(7.874007874011811, 67356364)
0
# 162
Accuracy: 0.931361536351166
{1: 65950751, 0: 6949249}
(7.937253933193772, 72265858)
1
# 163
Accuracy: 0.9310849108367627
{1: 66076665, 0: 6823335}
(7.874007874011811, 16430942)
1
# 164
Accuracy: 0.9314891358024692
{1: 66126391, 0: 6773609}
(8.366600265340756, 728194)
1
# 165
Accuracy: 0.932083854595336
{1: 65951564, 0: 6948436}
(7.874007874011811, 16698962)
0
# 166
Accuracy: 0.9327666392318245
{1: 66063493, 0: 6836507}
(7.874007874011811, 55470379)
1
# 167
Accuracy: 0.9364381755829904
{1: 65726456, 0: 7173544}
(7.810249675906654, 54085239)
0
# 168
Accuracy: 0.9369928120713306
{1: 65677135, 0: 7222865}
(7.937253933193772, 3598652)
1
# 169
Accuracy: 0.9335051989026063
{1: 66232784, 0: 6667216}
(7.745966692414834, 72827101)
0
# 170
Accuracy: 0.9352128943758573
{1: 65336179, 0: 7563821}
(7.874007874011811, 39446679)
1
# 171
Accuracy: 0.9361715775034294
{1: 65987503, 0: 6912497}
(7.745966692414834, 4327561)
0
# 172
Accuracy: 0.9350006035665295
{1: 65258079, 0: 7641921}
(8.0, 1603801)
1
# 173
Accuracy: 0.9363490397805213
{1: 65364688, 0: 7535312}
(7.745966692414834, 47604421)
1
# 174
Accuracy: 0.9371917421124829
{1: 65638267, 0: 7261733}
(7.681145747868608, 32803299)
0
# 175
Accuracy: 0.9375789163237311
{1: 65689484, 0: 7210516}
(8.12403840463596, 44447139)
1
# 176
Accuracy: 0.937709561042524
{1: 65701054, 0: 7198946}
(7.810249675906654, 67790079)
0
# 177
Accuracy: 0.9381213717421125
{1: 65738403, 0: 7161597}
(7.681145747868608, 48770178)
0
# 178
Accuracy: 0.9398945404663923
{1: 65326969, 0: 7573031}
(7.681145747868608, 72241209)
1
# 179
Accuracy: 0.9396164197530864
{1: 65427922, 0: 7472078}
(7.615773105863909, 2184934)
1
# 180
Accuracy: 0.9389087105624143
{1: 65060254, 0: 7839746}
(7.745966692414834, 33461491)
1
# 181
Accuracy: 0.9373782853223595
{1: 66015476, 0: 6884524}
(7.745966692414834, 68022183)
0
# 182
Accuracy: 0.9427649519890261
{1: 65721252, 0: 7178748}
(7.810249675906654, 67797369)
1
# 183
Accuracy: 0.9418254732510288
{1: 65724310, 0: 7175690}
(7.615773105863909, 69805038)
1
# 184
Accuracy: 0.9396836488340192
{1: 66015755, 0: 6884245}
(7.681145747868608, 8748002)
1
# 185
Accuracy: 0.9408217009602194
{1: 65522339, 0: 7377661}
(7.54983443527075, 1894599)
0
# 186
Accuracy: 0.9403720987654322
{1: 65516941, 0: 7383059}
(7.681145747868608, 1683186)
0
# 187
Accuracy: 0.9383078875171468
{1: 65944320, 0: 6955680}
(7.681145747868608, 67140909)
1
# 188
Accuracy: 0.9393246502057613
{1: 65641978, 0: 7258022}
(7.681145747868608, 6783798)
1
# 189
Accuracy: 0.9360650068587105
{1: 66177668, 0: 6722332}
(7.615773105863909, 51016182)
1
# 190
Accuracy: 0.9392754595336077
{1: 65939918, 0: 6960082}
(7.54983443527075, 6996699)
0
# 191
Accuracy: 0.9394977503429355
{1: 65828747, 0: 7071253}
(7.54983443527075, 32296056)
0
# 192
Accuracy: 0.94049341563786
{1: 65088749, 0: 7811251}
(7.810249675906654, 41844334)
1
# 193
Accuracy: 0.9398183264746228
{1: 65775265, 0: 7124735}
(7.54983443527075, 71113357)
0
# 194
Accuracy: 0.9380510425240055
{1: 65312916, 0: 7587084}
(7.54983443527075, 1678321)
1
# 195
Accuracy: 0.9388748010973937
{1: 65466826, 0: 7433174}
(7.483314773547883, 28729299)
1
# 196
Accuracy: 0.9377144444444444
{1: 66035656, 0: 6864344}
(7.483314773547883, 72820081)
0
# 197
Accuracy: 0.9392528669410151
{1: 65772065, 0: 7127935}
(7.483314773547883, 28073019)
0
# 198
Accuracy: 0.937759122085048
{1: 65554789, 0: 7345211}
(7.416198487095663, 2968569)
1
# 199
Accuracy: 0.9394573525377229
{1: 65658016, 0: 7241984}
(7.3484692283495345, 39906182)
0
# 200
Accuracy: 0.9401019890260631
{1: 65177390, 0: 7722610}
(7.615773105863909, 40532043)
1
# 201
Accuracy: 0.940512853223594
{1: 65604032, 0: 7295968}
(7.3484692283495345, 32360671)
1
# 202
Accuracy: 0.9403984224965707
{1: 65888652, 0: 7011348}
(7.416198487095663, 43514762)
1
# 203
Accuracy: 0.9381239094650206
{1: 66091746, 0: 6808254}
(7.3484692283495345, 54964716)
0
# 204
Accuracy: 0.9396895747599451
{1: 65671443, 0: 7228557}
(7.615773105863909, 3169356)
1
# 205
Accuracy: 0.9403875582990397
{1: 66068168, 0: 6831832}
(7.3484692283495345, 67726531)
0
# 206
Accuracy: 0.9398629355281207
{1: 65651023, 0: 7248977}
(7.483314773547883, 69221045)
1
# 207
Accuracy: 0.9386292729766804
{1: 65748799, 0: 7151201}
(7.280109889280518, 34233923)
0
# 208
Accuracy: 0.9402304663923182
{1: 65726166, 0: 7173834}
(7.416198487095663, 21810951)
1
# 209
Accuracy: 0.9389650068587105
{1: 65570848, 0: 7329152}
(7.280109889280518, 45879576)
0
# 210
Accuracy: 0.9402724965706447
{1: 65399592, 0: 7500408}
(7.280109889280518, 2186191)
1
# 211
Accuracy: 0.9415109190672154
{1: 65114965, 0: 7785035}
(7.416198487095663, 4598119)
1
# 212
Accuracy: 0.9429492181069958
{1: 65818373, 0: 7081627}
(7.483314773547883, 67075296)
1
# 213
Accuracy: 0.9416779561042524
{1: 65791472, 0: 7108528}
(7.3484692283495345, 2967759)
0
# 214
Accuracy: 0.9406091632373114
{1: 65661079, 0: 7238921}
(7.211102550927978, 4979339)
1
# 215
Accuracy: 0.9421329080932784
{1: 65759426, 0: 7140574}
(7.211102550927978, 6782406)
0
# 216
Accuracy: 0.9417249794238683
{1: 65467508, 0: 7432492}
(7.3484692283495345, 27635662)
1
# 217
Accuracy: 0.9436054320987655
{1: 65515471, 0: 7384529}
(7.211102550927978, 36084812)
1
# 218
Accuracy: 0.9431674897119342
{1: 65782715, 0: 7117285}
(7.14142842854285, 3971166)
1
# 219
Accuracy: 0.9424508093278464
{1: 65710345, 0: 7189655}
(7.14142842854285, 30952532)
0
# 220
Accuracy: 0.9438267626886145
{1: 65984110, 0: 6915890}
(7.3484692283495345, 31344032)
1
# 221
Accuracy: 0.9434476131687243
{1: 66125474, 0: 6774526}
(7.14142842854285, 17940426)
1
# 222
Accuracy: 0.9433417283950617
{1: 65962127, 0: 6937873}
(7.14142842854285, 29134805)
0
# 223
Accuracy: 0.9450374074074074
{1: 65849546, 0: 7050454}
(7.14142842854285, 69957309)
0
# 224
Accuracy: 0.943040438957476
{1: 65801137, 0: 7098863}
(7.280109889280518, 33508979)
1
# 225
Accuracy: 0.9452399314128944
{1: 65661450, 0: 7238550}
(7.211102550927978, 38411047)
1
# 226
Accuracy: 0.943015925925926
{1: 65788752, 0: 7111248}
(7.211102550927978, 50985229)
1
# 227
Accuracy: 0.9433271742112482
{1: 65915840, 0: 6984160}
(7.14142842854285, 6491826)
1
# 228
Accuracy: 0.9445465843621399
{1: 66020111, 0: 6879889}

threads 230
248
way
X
Y
(72900000, 8) (72900000,)
all {1: 63518691, 0: 9381309}
4665600112 583200096
# 230
Accuracy: 0.9469040466392318
{1: 65959398, 0: 6940602}
(7.14142842854285, 36379903)
0
# 231
Accuracy: 0.9460584087791495
{1: 65568439, 0: 7331561}
(7.14142842854285, 1685612)
1
# 232
Accuracy: 0.9448171604938271
{1: 65621368, 0: 7278632}
(7.211102550927978, 72396543)
0
# 233
Accuracy: 0.9440567489711934
{1: 65486640, 0: 7413360}
(7.3484692283495345, 70348906)
1
# 234
Accuracy: 0.9459422496570645
{1: 65683019, 0: 7216981}
(7.211102550927978, 43033321)
1
# 235

'''








